{
  "testexp": {
    "title": "Generating epitope-specific TCR sequences using s pre-trained protein language model",
    "description": "Generating epitope-specific TCR sequences using s pre-trained protein language model",
    "paper": "ASAP",
    "output_dir": "../output/testexp",
    "taskflow": {
      "mlm_finetune": {
        "type": "mlm_finetune",
        "plm_name_or_path": "facebook/esm2_t33_650M_UR50D",
        "peft": {
          "bits": 8
        },
        "data": {
          "config": "../config/data-test.json",
          "data_key": "immunecode",
          "val_size": 0.2,
          "seq_mutators": {
            "epitope": {
              "type": "calis",
              "mut_ratio": 0.15,
              "mut_probs": [0.7, 0.3]
            },
            "target": {
              "type": "uniform",
              "mut_ratio": 0.15,
              "mut_probs": [0.8, 0.2]
            }
          }
        },
        "trainer": {
          "args": {
            "dataloader_num_workers": 4,
            "dataloader_pin_memory": true,
            "per_device_train_batch_size": 32,
            "per_device_eval_batch_size": 32,
            "gradient_accumulation_steps": 4,
            "gradient_checkpointing": true,
            "lr_scheduler_type": "polynomial",
            "warmup_ratio": 0.2,
            "weight_decay": 0.01,
            "num_train_epochs": 10,
            "learning_rate": 2e-4,
            "fp16": true,
            "evaluation_strategy": "steps",
            "eval_steps": 0.01,
            "label_names": ["labels"],
            "load_best_model_at_end": true,
            "metric_for_best_model": "eval_loss",
            "save_strategy": "steps",
            "save_steps": 0.01,
            "logging_strategy": "steps",
            "logging_steps": 0.01,
            "optim": "paged_adamw_8bit",
            "output_dir": "{task_output_dir}"
          },
          "callbacks": [
            {
              "type": "EarlyStoppingCallback",
              "early_stopping_patience": 2
            }
          ]
        }
      }
    }
  },
  "exp1": {
    "title": "Generating SARS-CoV2 T-cell epitope-specific TCR sequences using s pre-trained protein language model",
    "description": "Generating SARS-CoV2 T-cell epitope-specific TCR sequences using s pre-trained protein language model",
    "paper": "ASAP",
    "output_dir": "../output/exp1",
    "taskflow": {
      "mlm_finetune": {
        "type": "mlm_finetune",
        "plm_name_or_path": "facebook/esm2_t33_650M_UR50D",
        "peft": {
          "bits": 4
        },
        "data": {
          "config": "../config/data.json",
          "data_key": "immunecode",
          "val_size": 0.2,
          "seq_mutators": {
            "epitope": {
              "type": "calis",
              "mut_ratio": 0.15,
              "mut_probs": [0.7, 0.3]
            },
            "target": {
              "type": "uniform",
              "mut_ratio": 0.4,
              "mut_probs": [0.8, 0.2]
            }
          }
        },
        "trainer": {
          "args": {
            "dataloader_num_workers": 12,
            "dataloader_pin_memory": true,
            "per_device_train_batch_size": 2048,
            "per_device_eval_batch_size": 2048,
            "gradient_accumulation_steps": 4,
            "gradient_checkpointing": true,
            "lr_scheduler_type": "polynomial",
            "warmup_ratio": 0.2,
            "weight_decay": 0.01,
            "num_train_epochs": 100,
            "learning_rate": 2e-4,
            "fp16": true,
            "evaluation_strategy": "steps",
            "eval_steps": 0.01,
            "label_names": ["labels"],
            "load_best_model_at_end": true,
            "metric_for_best_model": "eval_loss",
            "save_strategy": "steps",
            "save_steps": 0.01,
            "logging_strategy": "steps",
            "logging_steps": 0.01,
            "optim": "paged_adamw_8bit",
            "output_dir": "{task_output_dir}"
          },
          "callbacks": [
            {
              "type": "EarlyStoppingCallback",
              "early_stopping_patience": 10
            }
          ]
        }
      }
    }
  },
  "exp2": {
    "title": "Modifying the mutation properties of exp1 configuration",
    "description": "Modifying the mutation properties of exp1 configuration",
    "paper": "ASAP",
    "output_dir": "../output/exp2",
    "taskflow": {
      "mlm_finetune": {
        "type": "mlm_finetune",
        "plm_name_or_path": "facebook/esm2_t33_650M_UR50D",
        "peft": {
          "bits": 8
        },
        "data": {
          "config": "../config/data.json",
          "data_key": "immunecode",
          "val_size": 0.2,
          "seq_mutators": {
            "epitope":{
              "type": "calis",
              "mut_ratio": 0.15,
              "mut_probs": [0.7, 0.3]
            },
            "target":{
              "type": "uniform",
              "mut_ratio": 0.2,
              "mut_probs": [0.8, 0.2]
            }
          }
        },
        "trainer": {
          "args": {
            "dataloader_num_workers": 24,
            "dataloader_pin_memory": true,
            "per_device_train_batch_size": 2048,
            "per_device_eval_batch_size": 2048,
            "gradient_accumulation_steps": 4,
            "gradient_checkpointing": true,
            "lr_scheduler_type": "polynomial",
            "warmup_ratio": 0.2,
            "weight_decay": 0.01,
            "num_train_epochs": 50,
            "learning_rate": 2e-4,
            "fp16": true,
            "evaluation_strategy": "steps",
            "eval_steps": 0.01,
            "label_names": ["labels"],
            "load_best_model_at_end": true,
            "metric_for_best_model": "eval_loss",
            "save_strategy": "steps",
            "save_steps": 0.01,
            "logging_strategy": "steps",
            "logging_steps": 0.01,
            "optim": "paged_adamw_8bit",
            "output_dir": "{task_output_dir}"
          },
          "callbacks": [
            {
              "type": "EarlyStoppingCallback",
              "early_stopping_patience": 5
            }
          ]
        }
      }
    }
  },
  "exp3": {
    "title": "exp3",
    "description": "Making the mutaion properties of TCR CDR3beta and epitope sequences the same",
    "paper": "ASAP",
    "output_dir": "../output/exp3",
    "taskflow": {
      "mlm_finetune": {
        "type": "mlm_finetune",
        "plm_name_or_path": "facebook/esm2_t33_650M_UR50D",
        "peft": {
          "bits": 8
        },
        "data": {
          "config": "../config/data.json",
          "data_key": "immunecode",
          "val_size": 0.2,
          "seq_mutators": {
            "epitope":{
              "type": "calis",
              "mut_ratio": 0.15,
              "mut_probs": [0.7, 0.3]
            },
            "target":{
              "type": "uniform",
              "mut_ratio": 0.15,
              "mut_probs": [0.7, 0.3]
            }
          }
        },
        "trainer": {
          "args": {
            "dataloader_num_workers": 24,
            "dataloader_pin_memory": true,
            "per_device_train_batch_size": 2048,
            "per_device_eval_batch_size": 2048,
            "gradient_accumulation_steps": 4,
            "gradient_checkpointing": true,
            "lr_scheduler_type": "polynomial",
            "warmup_ratio": 0.2,
            "weight_decay": 0.01,
            "num_train_epochs": 50,
            "learning_rate": 2e-4,
            "fp16": true,
            "evaluation_strategy": "steps",
            "eval_steps": 0.01,
            "label_names": ["labels"],
            "load_best_model_at_end": true,
            "metric_for_best_model": "eval_loss",
            "save_strategy": "steps",
            "save_steps": 0.01,
            "logging_strategy": "steps",
            "logging_steps": 0.01,
            "optim": "paged_adamw_8bit",
            "output_dir": "{task_output_dir}"
          },
          "callbacks": [
            {
              "type": "EarlyStoppingCallback",
              "early_stopping_patience": 5
            }
          ]
        }
      }
    }
  },
  "exp4": {
    "title": "exp4",
    "description": "Mutation ratio: 0.15 for both epitope and TCR CDR3beta sequences",
    "paper": "ASAP",
    "output_dir": "../output/exp4",
    "taskflow": {
      "mlm_finetune": {
        "type": "mlm_finetune",
        "plm_name_or_path": "facebook/esm2_t33_650M_UR50D",
        "peft": {
          "bits": 8
        },
        "data": {
          "config": "../config/data.json",
          "data_key": "immunecode",
          "val_size": 0.2,
          "seq_mutators": {
            "epitope":{
              "type": "calis",
              "mut_ratio": 0.15,
              "mut_probs": [0.7, 0.3]
            },
            "target":{
              "type": "uniform",
              "mut_ratio": 0.15,
              "mut_probs": [0.7, 0.3]
            }
          }
        },
        "trainer": {
          "args": {
            "dataloader_num_workers": 24,
            "dataloader_pin_memory": true,
            "per_device_train_batch_size": 2048,
            "per_device_eval_batch_size": 2048,
            "gradient_accumulation_steps": 4,
            "gradient_checkpointing": true,
            "lr_scheduler_type": "polynomial",
            "warmup_ratio": 0.2,
            "weight_decay": 0.01,
            "num_train_epochs": 30,
            "learning_rate": 2e-4,
            "fp16": true,
            "evaluation_strategy": "steps",
            "eval_steps": 0.01,
            "label_names": ["labels"],
            "load_best_model_at_end": true,
            "metric_for_best_model": "eval_loss",
            "save_strategy": "steps",
            "save_steps": 0.01,
            "logging_strategy": "steps",
            "logging_steps": 0.01,
            "optim": "paged_adamw_8bit",
            "output_dir": "{task_output_dir}"
          },
          "callbacks": [
            {
              "type": "EarlyStoppingCallback",
              "early_stopping_patience": 3
            }
          ]
        }
      }
    }
  }
}
